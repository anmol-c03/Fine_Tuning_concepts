
# Fine-Tuning Concepts
# Table of Contents
1. [Project Overview](#project-overview)
2. [Usage](#Usage)
3. [TODO] (#TODO)
4. [References](#references)

## Project Overview

This project explores the concept of instruction tuning and quantizations techniques. Bitnet1.58 is the quantization technique which reduces weights to be either -1, 0, 1. Moreover, this repo implements LORA which gives us vivid understanding of how adapters are integrated and how lora reduces computational cost. MOreover , llaama2 is finetuned with 'databricks-dolly-15k' dataset to produce a fine-tuned chatbot. 

## Usage 
One can use this repo to have a clear understanding of instruction tuning and quantization techniques. Moreover, you can use this repository to learn more about post-training.

## TODO
1. implemet performance enhancement techniques like DPO
2. uncensoring LLMS using abliterations
3. Explore more about quantization techniques

## References
LORA  https://arxiv.org/pdf/2106.09685

Bitnet1.58 https://arxiv.org/pdf/2402.17764



